{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Jules\\.virtualenvs\\Energy_Generation-GIFzPpK6\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Notebook used to prototype different xgboost models\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import seaborn\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "from skopt.space import Real, Integer\n",
    "from skopt import BayesSearchCV\n",
    "import requests\n",
    "import json\n",
    "from entsoe import EntsoePandasClient\n",
    "\n",
    "from skforecast.ForecasterBaseline import ForecasterEquivalentDate\n",
    "from skforecast.ForecasterAutoreg import ForecasterAutoreg\n",
    "from skforecast.model_selection import bayesian_search_forecaster\n",
    "from skforecast.model_selection import backtesting_forecaster\n",
    "from skforecast.utils import check_y\n",
    "\n",
    "from astral.sun import sun\n",
    "from astral import LocationInfo\n",
    "import data\n",
    "\n",
    "from sklearn.linear_model import LinearRegression,Ridge\n",
    "import xgbmodel as xgb\n",
    "import pickle\n",
    "from datetime import timedelta,datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2015 - 2018 dataset: \"../datasets/energy_dataset.csv\"\n",
    "\n",
    "df2 = data.load_data(dataset = \"../datasets/energy_updated.csv\")\n",
    "df2 = data.preprocessing(df2)\n",
    "df2.rename({\"Actual Load\":\"total load actual\"},axis=1, inplace=True)\n",
    "df2.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dates for other dataset: \"2017-06-30 23:00:00\",'2018-03-31 23:00:00'\n",
    "\n",
    "trainx,testx,trainy,testy, end_validation = data.split_data(df2,\"total load actual\",\"2023-03-01 23:00:00\",'2023-06-30 23:00:00')\n",
    "\n",
    "testx.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fourier encoding for calender features\n",
    "def fourier_features(feature,cycle_length,order):\n",
    "    result = pd.DataFrame()\n",
    "\n",
    "    k = 2 * np.pi * feature/cycle_length\n",
    "    for i in range(1,order+1):\n",
    "        result[f\"sin_{feature.name}_{i}\"] =  np.sin(i*k)\n",
    "        result[f\"cos_{feature.name}_{i}\"]    =  np.cos(i*k)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import weather features from dataset\n",
    "weather_df = pd.read_csv(\"../datasets/weather_features.csv\")\n",
    "weather_df = weather_df.loc[weather_df[\"city_name\"] == \"Madrid\"]\n",
    "\n",
    "#Drop duplicates from index\n",
    "weather_df = weather_df.drop_duplicates(subset=\"dt_iso\")\n",
    "\n",
    "dups = weather_df[\"dt_iso\"].duplicated()\n",
    "dups.loc[dups == False]\n",
    "\n",
    "weather_df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract rolling window temperature features: min, max and avg temperature\n",
    "temp_features = weather_df[\"temp\"].copy()\n",
    "temp_features = temp_features.to_frame()\n",
    "\n",
    "temp_features['temp_roll_mean_1_day'] = temp_features['temp'].rolling(24, closed='left').mean()\n",
    "temp_features['temp_roll_mean_7_day'] = temp_features['temp'].rolling(24*7, closed='left').mean()\n",
    "temp_features['temp_roll_max_1_day'] = temp_features['temp'].rolling(24, closed='left').max()\n",
    "temp_features['temp_roll_min_1_day'] = temp_features['temp'].rolling(24, closed='left').min()\n",
    "temp_features['temp_roll_max_7_day'] = temp_features['temp'].rolling(24*7, closed='left').max()\n",
    "temp_features['temp_roll_min_7_day'] = temp_features['temp'].rolling(24*7, closed='left').min()\n",
    "\n",
    "\n",
    "temp_features.index = df2.index\n",
    "\n",
    "temp_features.head\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = LocationInfo(\n",
    "    name='Washington DC',\n",
    "    region='Spain',\n",
    "    timezone='CET')\n",
    "\n",
    "calendar_features = pd.DataFrame(index=df2.index)\n",
    "calendar_features['month'] = calendar_features.index.month\n",
    "calendar_features['week_of_year'] = calendar_features.index.isocalendar().week\n",
    "calendar_features['week_day'] = calendar_features.index.day_of_week + 1\n",
    "calendar_features['hour_day'] = calendar_features.index.hour + 1\n",
    "sunrise_hour = [\n",
    "    sun(location.observer, date=date, tzinfo=location.timezone)['sunrise'].hour\n",
    "    for date in df2.index\n",
    "]\n",
    "sunset_hour = [\n",
    "    sun(location.observer, date=date, tzinfo=location.timezone)['sunset'].hour\n",
    "    for date in df2.index\n",
    "]\n",
    "sun_light_features = pd.DataFrame({\n",
    "                         'sunrise_hour': sunrise_hour,\n",
    "                         'sunset_hour': sunset_hour}, \n",
    "                         index = df2.index\n",
    "                     )\n",
    "sun_light_features['daylight_hours'] = (\n",
    "    sun_light_features['sunset_hour'] - sun_light_features['sunrise_hour']\n",
    ")\n",
    "sun_light_features['is_daylight'] = np.where(\n",
    "                                        (df2.index.hour >= sun_light_features['sunrise_hour']) & \\\n",
    "                                        (df2.index.hour < sun_light_features['sunset_hour']),\n",
    "                                        1,\n",
    "                                        0\n",
    "                                    )\n",
    "exo_features = pd.concat([\n",
    "                            calendar_features,\n",
    "                            sun_light_features,\n",
    "                         \n",
    "                        ], axis=1)\n",
    "\n",
    "month_encoded = fourier_features(exo_features[\"month\"], 12,1)\n",
    "week_of_year_encoded = fourier_features(exo_features['week_of_year'], 52,1)\n",
    "week_day_encoded = fourier_features(exo_features['week_day'], 7,1)\n",
    "hour_day_encoded = fourier_features(exo_features['hour_day'], 24,1)\n",
    "cyclical_features = pd.concat([\n",
    "                        month_encoded,\n",
    "                        week_of_year_encoded,\n",
    "                        week_day_encoded,\n",
    "                        hour_day_encoded,\n",
    "                    ], axis=1)\n",
    "\n",
    "#cyclical_features = pd.concat([cyclical_features,temp_features], axis = 1)\n",
    "exo_features = pd.concat([exo_features, cyclical_features], axis=1)\n",
    "exo_features.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer_poly = PolynomialFeatures(\n",
    "                       degree           = 2,\n",
    "                       interaction_only = True,\n",
    "                       include_bias     = False,\n",
    "                       \n",
    "\n",
    "                   ).set_output(transform=\"pandas\")\n",
    "\"\"\"    'sin_sunrise_hour_1',\n",
    "    'cos_sunrise_hour_1',\n",
    "    'sin_sunset_hour_1',\n",
    "    'cos_sunset_hour_1',\"\"\"\n",
    "poly_cols = [\n",
    "    'sin_month_1', \n",
    "    'cos_month_1',\n",
    "    'sin_week_of_year_1',\n",
    "    'cos_week_of_year_1',\n",
    "    'sin_week_day_1', \n",
    "    'cos_week_day_1',\n",
    "    'sin_hour_day_1',\n",
    "    'cos_hour_day_1',\n",
    "    'daylight_hours',\n",
    "    'is_daylight',\n",
    "    #'temp_roll_mean_1_day',\n",
    "    #'temp_roll_mean_7_day',\n",
    "    #'temp_roll_max_1_day',\n",
    "    #'temp_roll_min_1_day',\n",
    "    #'temp_roll_max_7_day',\n",
    "    #'temp_roll_min_7_day',\n",
    "    #'temp'\n",
    "\n",
    "]\n",
    "\n",
    "poly_features = transformer_poly.fit_transform(exo_features[poly_cols].dropna())\n",
    "poly_features = poly_features.drop(columns=poly_cols)\n",
    "poly_features.columns = [f\"poly_{col}\" for col in poly_features.columns]\n",
    "poly_features.columns = poly_features.columns.str.replace(\" \", \"__\")\n",
    "\n",
    "exo_features = pd.concat([exo_features, poly_features], axis=1)\n",
    "exo_features.head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "features = []\n",
    "\n",
    "# Select the columns to be used as exo features\n",
    "#features.extend(exo_features.columns.tolist())\n",
    "\n",
    "features.extend(exo_features.filter(regex='^sin_|^cos_').columns.tolist())\n",
    "\n",
    "features.extend(exo_features.filter(regex='^temp_.*').columns.tolist())\n",
    "\n",
    "#add or remove temp features \n",
    "features = [x for x in features if \"temp\" not in x]\n",
    "\n",
    "df2 = df2[[\"total load actual\"]].merge(\n",
    "           exo_features,\n",
    "           left_index=True,\n",
    "           right_index=True,\n",
    "           how='left'\n",
    "       )\n",
    "\n",
    "df2.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skforecast\n",
    "forecast = ForecasterAutoreg(regressor = XGBRegressor(random_state = 1543),lags = 168)\n",
    "forecast.fit(y=trainy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "metric, predictions = backtesting_forecaster(\n",
    "                          forecaster         = forecast,\n",
    "                          y                  = df2[\"total load actual\"],\n",
    "                          steps              = 24,\n",
    "                          metric             =  'mean_absolute_error',\n",
    "                          initial_train_size = len(trainy),\n",
    "                          refit              = False,\n",
    "                          n_jobs             = 'auto',\n",
    "                          verbose            = True, \n",
    "                          show_progress      = True\n",
    "                      )\n",
    "\n",
    "print(f'Backtest error (MAE): {metric}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction without using exogenous variables \n",
    "\n",
    "lags_grid = [48, 72, [1, 2, 3, 23, 24, 25, 167, 168, 169]]  \n",
    "\n",
    "def search_space(trial):\n",
    "    search_space  = {\n",
    "        'n_estimators'  : trial.suggest_int('n_estimators', 400, 1200, step=100),\n",
    "        'max_depth'     : trial.suggest_int('max_depth', 3, 10, step=1),\n",
    "        'learning_rate' : trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'reg_alpha'     : trial.suggest_float('reg_alpha', 0, 1, step=0.1),\n",
    "        'reg_lambda'    : trial.suggest_float('reg_lambda', 0, 1, step=0.1),\n",
    "    } \n",
    "    return search_space\n",
    "search_params = {\n",
    "    \"reg__max_depth\": Integer(2,8),\n",
    "    \"reg__learning_rate\": Real(0.001,0.1,prior=\"log-uniform\"),\n",
    "    \"reg__subsample\": Real(0.5,1.0),\n",
    "    \"reg__reg_alpha\": Real(0.0,10.0),\n",
    "    \"reg__reg_lambda\": Real(0.0,10.0),\n",
    "    \"reg__gamma\": Real(0.0,10.0)\n",
    "\n",
    "}\n",
    "\n",
    "results_search, frozen_trial = bayesian_search_forecaster(\n",
    "                                   forecaster         = forecast,\n",
    "                                   y                  = df2[\"total load actual\"],\n",
    "                                   search_space       = search_space,\n",
    "                                   lags_grid          = lags_grid,\n",
    "                                   steps              = 24,\n",
    "                                   refit              = False,\n",
    "                                   metric             = 'mean_absolute_error',\n",
    "                                   initial_train_size = len(trainy),\n",
    "                                   fixed_train_size   = False,\n",
    "                                   n_trials           = 10, \n",
    "                                   random_state       = 123,\n",
    "                                   return_best        = True,\n",
    "                                   n_jobs             = 'auto',\n",
    "                                   verbose            = False,\n",
    "                                   show_progress      = True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10201"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df2.loc[: \"2023-03-01 23:00:00\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model trained using exogenous features\n",
    "\n",
    "\n",
    "forecast = ForecasterAutoreg(regressor = XGBRegressor(random_state = 1543),lags =169)\n",
    "forecast.fit(y=trainy)\n",
    "\n",
    "# Lags gridy\n",
    "lags_grid = [[1, 2, 3, 23, 24, 25, 167, 168, 169]]\n",
    "\n",
    "# Regressor hyperparameters search space\n",
    "def search_space(trial):\n",
    "    search_space  = {\n",
    "        'n_estimators' : trial.suggest_int('n_estimators', 800, 1400, step=100),\n",
    "        'max_depth'    : trial.suggest_int('max_depth', 3, 8, step=1),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.5),\n",
    "        'reg_alpha'    : trial.suggest_float('reg_alpha', 0, 1, step=0.1),\n",
    "        'reg_lambda'   : trial.suggest_float('reg_lambda', 0, 1, step=0.1),\n",
    "    } \n",
    "    return search_space\n",
    "\n",
    "results_search, frozen_trial = bayesian_search_forecaster(\n",
    "                                   forecaster         = forecast,\n",
    "                                   y                  = df2.loc[:end_validation, \"total load actual\"],\n",
    "                                   exog               = df2.loc[:end_validation, features],\n",
    "\n",
    "                                   search_space       = search_space,\n",
    "                                   lags_grid          = lags_grid,\n",
    "                                   steps              = 36,\n",
    "                                   refit              = False,\n",
    "                                   metric             = 'mean_absolute_error',\n",
    "\n",
    "                                   initial_train_size = len(df2.loc[: \"2023-03-01 23:00:00\"]),\n",
    "                                   fixed_train_size   = False,\n",
    "                                   n_trials           = 20,\n",
    "                                   random_state       = 123,\n",
    "                                   return_best        = True,\n",
    "                                   n_jobs             = 'auto',\n",
    "                                   verbose            = False,\n",
    "                                   show_progress      = True\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#backtesting using model with exogenous features\n",
    "metric, predictions = backtesting_forecaster(\n",
    "                          forecaster         = forecast,\n",
    "                          y                  = df2[\"total load actual\"],\n",
    "                          exog               = df2[features],\n",
    "                          steps              = 7,\n",
    "                          metric             = 'mean_absolute_error',\n",
    "                          initial_train_size = len(df2.loc[: end_validation]),\n",
    "                          refit              = False,\n",
    "                          n_jobs             = 'auto',\n",
    "                          verbose            = True, \n",
    "                          show_progress      = True\n",
    "                      )\n",
    "\n",
    "print(f\"Backtest error: {metric:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted2 = forecast.predict(steps=24,exog = df2.loc[\"2018-04-01 00:00:00+00:00\":,features])\n",
    "\n",
    "data2 = pd.concat([df2.loc[\"2018-04-01 00:00:00+00:00\":\"2018-04-02 00:00:00+00:00\",\"total load actual\"],predicted2],axis=1)\n",
    "\n",
    "fig,ax = plt.subplots(figsize=[20,5])\n",
    "seaborn.lineplot(ax = ax, data = data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates an XGBoost model without temperature features using the XGBoost class\n",
    "\n",
    "df2 = data.load_data(dataset = \"datasets/energy_updated.csv\")\n",
    "df2 = data.preprocessing(df2)\n",
    "\n",
    "poly_cols =     ['sin_month_1', \n",
    "        'cos_month_1',\n",
    "        'sin_week_of_year_1',\n",
    "        'cos_week_of_year_1',\n",
    "        'sin_week_day_1',\n",
    "        'cos_week_day_1',\n",
    "        'sin_hour_day_1',\n",
    "        'cos_hour_day_1',\n",
    "        'daylight_hours',\n",
    "        'is_daylight']\n",
    "\n",
    "model2 = xgb.xgboost_model(df2,\"total load actual\",\"2023-03-01 23:00:00\",'2023-06-30 23:00:00')\n",
    " \n",
    "model2.train_models(poly_cols)\n",
    "\n",
    "model2.backtesting()\n",
    "\n",
    "model2.save_model(\"xgboost_v2_no_temp.joblib\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('Energy_Generation-GIFzPpK6')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "201b8dff2e4c8c134d0877b5f008017789b0b414666b7ead251a12a07ca1a43f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
